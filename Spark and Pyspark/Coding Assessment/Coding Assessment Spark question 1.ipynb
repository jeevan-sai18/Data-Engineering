{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "479856e8-7d1e-4a59-8558-8260295a80d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CodingAssessment\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150cfc6e-c9c7-4bc2-a346-783e28c6cad8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n|  A|  B|\n+---+---+\n|  1|  a|\n|  2|  b|\n|  3|  c|\n|  4|  d|\n+---+---+\n\n+---+---+\n|  A|  C|\n+---+---+\n|  3|  x|\n|  4|  y|\n|  5|  z|\n|  6|  w|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Create sample DataFrames\n",
    "data1 = [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]\n",
    "df1 = spark.createDataFrame(data1, ['A', 'B'])\n",
    "\n",
    "data2 = [(3, 'x'), (4, 'y'), (5, 'z'), (6, 'w')]\n",
    "df2 = spark.createDataFrame(data2, ['A', 'C'])\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a3b532-5e99-4bbe-8012-3f618d71e5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified DataFrame 1:\n+---+---+\n|  A|  B|\n+---+---+\n|  2|  a|\n|  4|  b|\n|  6|  c|\n|  8|  d|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Manipulating DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "df1 = df1.withColumn('A', col('A') * 2)\n",
    "print(\"Modified DataFrame 1:\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526bc70c-7e9d-4db3-a80a-232b4b3e2bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDataFrame 2 after dropping column 'C':\n+---+\n|  A|\n+---+\n|  3|\n|  4|\n|  5|\n|  6|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Dropping columns\n",
    "df2 = df2.drop('C')\n",
    "print(\"\\nDataFrame 2 after dropping column 'C':\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb9c777-2c79-45cc-a15f-e684ecfb352b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDataFrame 1 sorted by column 'A':\n+---+---+\n|  A|  B|\n+---+---+\n|  2|  a|\n|  4|  b|\n|  6|  c|\n|  8|  d|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sorting DataFrame\n",
    "df1 = df1.orderBy('A')\n",
    "print(\"\\nDataFrame 1 sorted by column 'A':\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e226fc8d-799b-48b4-8efe-34804b4e6124",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n|  A|  B|\n+---+---+\n|  8|  d|\n|  6|  c|\n|  4|  b|\n|  2|  a|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Sorting DataFrame in descending order\n",
    "df1 = df1.orderBy(col('A').desc())\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccbbf63-810b-4736-9194-5bdb7a7c691f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMean of column 'A' in DataFrame 1: 5.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Aggregations\n",
    "mean_A = df1.select(avg('A')).collect()[0][0]\n",
    "print(\"\\nMean of column 'A' in DataFrame 1:\", mean_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5525e792-1cc6-4ac8-bc68-6e69ce37193b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nMerged DataFrame:\n+---+---+\n|  A|  B|\n+---+---+\n|  4|  b|\n|  6|  c|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "# Joining DataFrames\n",
    "df_merged = df1.join(df2, on='A', how='inner')\n",
    "print(\"\\nMerged DataFrame:\")\n",
    "df_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e6feec-9852-4ffe-b256-a1328981306c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nGrouped DataFrame:\n+---+------+\n|  B|sum(A)|\n+---+------+\n|  c|     6|\n|  b|     4|\n+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Grouping by DataFrame\n",
    "grouped = df_merged.groupBy('B').sum('A')\n",
    "print(\"\\nGrouped DataFrame:\")\n",
    "grouped.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Coding Assessment Spark question 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
