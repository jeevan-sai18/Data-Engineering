{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39db3431-0442-4e63-9a33-2adcb78e49c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| chandu|Data Science| 10000|\n| chandu|         IOT|  5000|\n| chandu|    Big Data|  4000|\n| Rohith|    Big Data| 40001|\n| Rohith|Data Science|  3000|\n|krishna|Data Science| 20000|\n|krishna|         IOT| 10000|\n|krishna|    Big Data|  5000|\n| rashmi|Data Science|  2000|\n| rashmi|    Big Data| 10000|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "spark =SparkSession.builder.appName(\"Notebook\").getOrCreate()\n",
    "df_pyspark=spark.read.csv(\"/FileStore/tables/file2_csv.txt\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30573c2-5f57-461d-9507-7717c62cb0a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n| Departments|sum(Salary)|\n+------------+-----------+\n|         IOT|      15000|\n|    Big Data|      10000|\n|    Big Data|      49001|\n|Data Science|      35000|\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#GROUPBY() fun with sum\n",
    "df_pyspark.groupBy(\"Departments\").sum(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f7537b-f92e-474b-b677-a036bf5c9c73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n| Departments|count|\n+------------+-----+\n|         IOT|    2|\n|    Big Data|    1|\n|    Big Data|    3|\n|Data Science|    4|\n+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# COUNT() Function\n",
    "df_pyspark.groupBy(\"Departments\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4ae65e5-5c5b-48d0-829d-cfb2c9b21ff5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n| Departments|min(salary)|\n+------------+-----------+\n|         IOT|       5000|\n|    Big Data|      10000|\n|    Big Data|       4000|\n|Data Science|       2000|\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Departments\").min(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2310ad1c-18ba-4f16-a63e-05be3aaafb20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n| Departments|max(salary)|\n+------------+-----------+\n|         IOT|      10000|\n|    Big Data|      10000|\n|    Big Data|      40001|\n|Data Science|      20000|\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(\"Departments\").max(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d458b708-e5e7-434a-adf7-d8cc9fc31d82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n| Departments|       avg(Salary)|\n+------------+------------------+\n|         IOT|            7500.0|\n|    Big Data|           10000.0|\n|    Big Data|16333.666666666666|\n|Data Science|            8750.0|\n+------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# MEAN() Function\n",
    "df_pyspark.groupBy(\"Departments\").mean(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f289ab09-24e6-4d14-9f23-44db78795b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n| Departments|       avg(Salary)|\n+------------+------------------+\n|         IOT|            7500.0|\n|    Big Data|           10000.0|\n|    Big Data|16333.666666666666|\n|Data Science|            8750.0|\n+------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# AVG() Function\n",
    "df_pyspark.groupBy(\"Departments\").avg(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3546ed23-5380-4993-bbaa-68a2f61a1c17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+------+-------+------+\n| Departments|Rohith|chandu|krishna|rashmi|\n+------------+------+------+-------+------+\n|         IOT|  NULL|  5000|  10000|  NULL|\n|    Big Data|  NULL|  NULL|   NULL| 10000|\n|    Big Data| 40001|  4000|   5000|  NULL|\n|Data Science|  3000| 10000|  20000|  2000|\n+------------+------+------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Pivot Function\n",
    "df_pyspark.groupBy(\"Departments\").pivot(\"Name\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c0ca5c-baf5-4182-afeb-33e3d169082b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------+\n|   Name| Departments|sum(salary)|\n+-------+------------+-----------+\n| rashmi|    Big Data|      10000|\n| Rohith|Data Science|       3000|\n| Rohith|    Big Data|      40001|\n| rashmi|Data Science|       2000|\n| chandu|         IOT|       5000|\n| chandu|    Big Data|       4000|\n|krishna|    Big Data|       5000|\n| chandu|Data Science|      10000|\n|krishna|         IOT|      10000|\n|krishna|Data Science|      20000|\n+-------+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# groupby with multiple columns\n",
    "df_pyspark.groupBy(\"Name\",\"Departments\").sum(\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2969947-88bd-4633-b94b-425fdd9579d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n| Departments|sum(salary)|\n+------------+-----------+\n|         IOT|      15000|\n|    Big Data|      10000|\n|    Big Data|      49001|\n|Data Science|      35000|\n+------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#groupBy() and agg() function\n",
    "df_pyspark.groupBy(\"Departments\").agg(({\"salary\":\"sum\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fae445e-f7eb-4f4d-9e32-321942a72d52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|sum(salary)|\n+-----------+\n|     109001|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg(({\"salary\":\"sum\"})).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af726e69-9d1b-4155-bbb9-b03fda85dc7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|null|      null| 40000|\n|     null|  34|        10| 38000|\n|     null|  36|      null|  null|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "spark =SparkSession.builder.appName(\"Notebook\").getOrCreate()\n",
    "df_pyspark1=spark.read.csv(\"/FileStore/tables/file5.csv\",header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba46afe-13ad-4e82-9dab-b40580566f73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n|     Name|Age|Experience|Salary|\n+---------+---+----------+------+\n|    Krish| 31|        10| 30000|\n|Sudhanshu| 30|         8| 25000|\n|    Sunny| 29|         4| 20000|\n|     Paul| 24|         3| 20000|\n|   Harsha| 21|         1| 15000|\n|  Shubham| 23|         2| 18000|\n+---------+---+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows based on null values\n",
    "df_pyspark1.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05020290-48be-4fcf-bee9-617e9ec30cf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|NULL|      NULL| 40000|\n|     NULL|  34|        10| 38000|\n|     null|  36|      NULL|  NULL|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    " # if all values in rows are null then drop # default any\n",
    "df_pyspark1.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1160c345-4409-4c77-872f-56403bdc7323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|NULL|      NULL| 40000|\n|     NULL|  34|        10| 38000|\n|     null|  36|      NULL|  NULL|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#atleast 2 non null values should be present. \n",
    "df_pyspark1.na.drop(how=\"any\",thresh=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6315be0-f4e5-4cf6-8d5f-264c8e2ebe6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n|     Name| Age|Experience|Salary|\n+---------+----+----------+------+\n|    Krish|  31|        10| 30000|\n|Sudhanshu|  30|         8| 25000|\n|    Sunny|  29|         4| 20000|\n|     Paul|  24|         3| 20000|\n|   Harsha|  21|         1| 15000|\n|  Shubham|  23|         2| 18000|\n|   Mahesh|NULL|      NULL| 40000|\n|     NULL|  34|        10| 38000|\n+---------+----+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# only in that column rows get deleted\n",
    "df_pyspark1.na.drop(how=\"any\",subset=[\"salary\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cd5967-97d1-425f-91ca-b24e88aa9f22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| chandu|Data Science| 10000|\n| chandu|         IOT|  5000|\n| chandu|    Big Data|  4000|\n| Rohith|    Big Data| 40001|\n| Rohith|Data Science|  3000|\n|krishna|Data Science| 20000|\n|krishna|         IOT| 10000|\n|krishna|    Big Data|  5000|\n| rashmi|Data Science|  2000|\n| rashmi|    Big Data| 10000|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdcb0a8-cf7f-4f47-add5-cba8a3a9c179",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| rashmi|Data Science|  2000|\n| Rohith|Data Science|  3000|\n| chandu|    Big Data|  4000|\n| chandu|         IOT|  5000|\n|krishna|    Big Data|  5000|\n| chandu|Data Science| 10000|\n|krishna|         IOT| 10000|\n| rashmi|    Big Data| 10000|\n|krishna|Data Science| 20000|\n| Rohith|    Big Data| 40001|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort based on single column\n",
    "df_pyspark.sort(\"Salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ee5a35-b378-4c95-bf16-c9fe8237a027",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| Rohith|    Big Data| 40001|\n|krishna|Data Science| 20000|\n| chandu|Data Science| 10000|\n|krishna|         IOT| 10000|\n| rashmi|    Big Data| 10000|\n| chandu|         IOT|  5000|\n|krishna|    Big Data|  5000|\n| chandu|    Big Data|  4000|\n| Rohith|Data Science|  3000|\n| rashmi|Data Science|  2000|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#sort based on descending order\n",
    "df_pyspark.sort(df_pyspark[\"Salary\"].desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85cf204f-440f-47e6-a949-cfce5b290d51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| rashmi|Data Science|  2000|\n| Rohith|Data Science|  3000|\n| chandu|    Big Data|  4000|\n| chandu|         IOT|  5000|\n|krishna|    Big Data|  5000|\n| chandu|Data Science| 10000|\n|krishna|         IOT| 10000|\n| rashmi|    Big Data| 10000|\n|krishna|Data Science| 20000|\n| Rohith|    Big Data| 40001|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Sort based on first column then second column\n",
    "df_pyspark.sort(\"Salary\",\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b2f510-a81f-474a-a0e3-c83b509e80db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- emp_id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- superior_emp_id: long (nullable = true)\n |-- year_joined: string (nullable = true)\n |-- emp_dept_id: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n|     6|   Brown|              2|       2010|         50|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000),(2, \"Rose\",1 , \"2010\", \"20\",\"M\", 4000),(3,\"Williams\",1,\"2010\",\"10\",\"M\",1000),(4, \"Jones\",2 ,\"2005\",\"10\",\"F\",2000),(5,\"Brown\",2,\"2010\",\"40\",\"\",-1),(6, \"Brown\", 2, \"2010\",\"50\",\"\",-1)]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7ba878c-37a6-45b5-9883-ed0566589425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- dept_name: string (nullable = true)\n |-- dept_id: long (nullable = true)\n\n+---------+-------+\n|dept_name|dept_id|\n+---------+-------+\n|  Finance|     10|\n|Marketing|     20|\n|    Sales|     30|\n|       IT|     40|\n+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10),(\"Marketing\",20),(\"Sales\",30),(\"IT\",40)]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023f2f44-0f2c-4b1f-b628-29147856f146",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a633a870-3e61-4b7d-98b8-31f1d618b2b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32c675d-605c-4753-b640-ee14ed0f2276",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n|     6|   Brown|              2|       2010|         50|      |    -1|     NULL|   NULL|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5bb8fc4-001a-467d-be60-98fd86f127d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n|     1|   Smith|             -1|       2018|         10|     M|  3000|  Finance|     10|\n|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n|  NULL|    NULL|           NULL|       NULL|       NULL|  NULL|  NULL|    Sales|     30|\n|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e406f1-0c9b-4b22-86b7-cd8b69799749",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+--------+---------------+-----------+-----------+------+------+\n|     1|   Smith|             -1|       2018|         10|     M|  3000|\n|     3|Williams|              1|       2010|         10|     M|  1000|\n|     4|   Jones|              2|       2005|         10|     F|  2000|\n|     2|    Rose|              1|       2010|         20|     M|  4000|\n|     5|   Brown|              2|       2010|         40|      |    -1|\n+------+--------+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83b66808-be59-4898-88e9-3d9ecdef9f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n|emp_id| name|superior_emp_id|year_joined|emp_dept_id|gender|salary|\n+------+-----+---------------+-----------+-----------+------+------+\n|     6|Brown|              2|       2010|         50|      |    -1|\n+------+-----+---------------+-----------+-----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregate fun using spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
