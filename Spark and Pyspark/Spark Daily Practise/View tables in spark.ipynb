{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df702e3-e530-43a8-a06d-a00ba8bf1ee8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[database_description_item: string, database_description_value: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS customer_db;\")\n",
    "# Create a database named 'customer_db' with a comment and properties\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS customer_db COMMENT 'This is customer database' WITH DBPROPERTIES (ID='1', Name='John')\")\n",
    "\n",
    "# Describe the database to check if it's created successfully\n",
    "spark.sql(\"DESCRIBE DATABASE EXTENDED customer_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "026d2c25-93e0-4303-b152-8e6b77a26d3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66d490b-2e90-4833-b7c5-ca405179eb20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| chandu|Data Science| 10000|\n| chandu|         IOT|  5000|\n| chandu|    Big Data|  4000|\n| Rohith|    Big Data| 40001|\n| Rohith|Data Science|  3000|\n|krishna|Data Science| 20000|\n|krishna|         IOT| 10000|\n|krishna|    Big Data|  5000|\n| rashmi|Data Science|  2000|\n| rashmi|    Big Data| 10000|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"/FileStore/tables/file2_csv.txt\",header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa1f70b-5f21-4b65-adef-6403f47b8255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Name: string (nullable = true)\n |-- Departments: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48ac0401-bc6f-424d-af0d-4d103f91344c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|   Name|\n+-------+\n| chandu|\n| chandu|\n| chandu|\n| Rohith|\n| Rohith|\n|krishna|\n|krishna|\n|krishna|\n| rashmi|\n| rashmi|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604a8a94-d4e3-47e4-8752-eda429d5a9e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------+\n|   Name| Departments|Salary|\n+-------+------------+------+\n| chandu|Data Science| 10000|\n| chandu|         IOT|  5000|\n| chandu|    Big Data|  4000|\n| Rohith|    Big Data| 40001|\n| Rohith|Data Science|  3000|\n|krishna|Data Science| 20000|\n|krishna|         IOT| 10000|\n|krishna|    Big Data|  5000|\n| rashmi|Data Science|  2000|\n| rashmi|    Big Data| 10000|\n+-------+------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "View tables in spark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
